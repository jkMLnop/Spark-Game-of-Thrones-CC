from pyspark import SparkContext, SparkConf
import re

def cleanup_words(file_and_contents):
	split_contents = set()
	try:
	 	#break file and contents into tuple of file paths and memory address housing contents!
		filename,contents = file_and_contents
		#split_contents = set(re.split("\W+", body.lower()))
	except:
		print("Cleanup Error - problem encountered while attempting to clean file contents")
	#return(split_contents,filename)
	return(split_contents,"blah")

def process(input_file):
        #spark context setup
	spark_context = SparkContext(appName="reverse-index").getOrCreate()

	#generate an RDD of pair values files from input directory
	file_rdd = spark_context.wholeTextFiles(input_file)

	print("FILE RDD CREATED")

	#perform basic cleanup file contents (remove punctuation & make lower case)
	cleanup_rdd = file_rdd.map(cleanup_words).take(1)

def main(input_file, output_file):
	process(input_file)
	print("END OF MAIN REACHED")

if __name__ == '__main__':
	#define i/o file locations
	input_location = '../input/*'
	output_location = '../output/'
	
	#pass file paths to main 
	main(input_location, output_location)
